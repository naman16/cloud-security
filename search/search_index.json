{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Implementing CNAPP: Tool Selection and Day 1 Focus Areas","text":"<p>As someone who has done multiple Cloud-Native Application Protection Platform (CNAPP)  implementations, I've summarized my key considerations to help people who are in the evaluation / early stages of CNAPP adoption. Below is the high-level structure of how this document is organized:  </p> <ol> <li>What is CNAPP? </li> <li>Tool Selection Criteria </li> <li>Day 1 Focus Areas</li> <li>Closing Thoughts</li> </ol>"},{"location":"#what-is-cnapp","title":"What is CNAPP?","text":"<p>According to Gartner, \u201cCloud-native application protection platforms (CNAPPs) are a unified and tightly integrated set of security and compliance capabilities, designed to protect cloud-native infrastructure and applications. CNAPPs incorporate an integrated set of proactive and reactive security capabilities, including artifact scanning, security guardrails, configuration and compliance management, risk detection and prioritization, and behavioral analytics, providing visibility, governance and control from code creation to production runtime. CNAPP solutions use a combination of API integrations with leading cloud platform providers, continuous integration/continuous development (CI/CD) pipeline integrations, and agent and agentless workload integration to offer combined development and runtime security coverage.\u201d </p> <p>In other words, CNAPP is an umbrella term for multiple security capabilities that are integrated into a unified platform to secure cloud-native applications and infrastructure throughout their lifecycle. A typical CNAPP includes the following core capabilities:</p> <ul> <li>Cloud Security Posture Management (CSPM): Identifies misconfigurations and compliance risks in cloud infrastructure.  </li> <li>Cloud Infrastructure Entitlement Management (CIEM): Manages and automates access permissions in cloud environments.  </li> <li>Cloud Workload Protection Platform (CWPP): Protects cloud workloads, including containers, servers, and serverless functions.  </li> <li>Infrastructure-as-Code (IaC) Scanning: Detects security issues early in the development process.  </li> <li>Data Security Posture Management (DSPM): Discovers sensitive data sources and who has access to them.</li> </ul> <p>Below is an image from Gartner that provides a detailed view of the CNAPP capabilities landscape:</p> <p></p>"},{"location":"#tool-selection-criteria","title":"Tool Selection Criteria","text":"<p>When choosing a CNAPP solution, even though no vendor excels in all areas of CNAPP, I strongly advocate for a \"best-of-suite\" approach rather than assembling multiple \"best-of-breed\" tools. It is operationally easier to roll out across the organization, reduces the complexity associated with maintaining and managing various tools, and provides unified visibility across the different types of findings as it eliminates the need to correlate data from disparate tools. When evaluating CNAPP solutions, consider the following:</p> <ul> <li>Prioritize Vendors That Best Address Key Use-Cases: Identify the critical CNAPP capabilities required for your organization\u2019s cloud security strategy. My suggestion is to prioritize vendors with strong CSPM, vulnerability scanning, and CIEM features, even if other features are less developed, as long as they have a clear plan to support additional CNAPP capabilities of interest, such as CWPP, DSPM, attack surface management, software supply chain security, IaC scanning, container image scanning, etc.  </li> <li>Include Stakeholders From Different Teams: The capabilities that CNAPP offers extend to several different teams within the organization (Vulnerability Management, IAM, Security Monitoring, Data Protection), and including them in the selection process enables the organization to make a collective decision and helps to minimize the potential politics that can occur when key parties are left out of the decision-making process.  </li> <li>Require Graph Databases: Given the dynamic and interconnectedness of modern cloud environments where complex relationships exist between resources, identities, configurations, and data flows, traditional relational databases can't keep up with modeling and analyzing these intricate webs of connections. This is where graph databases shine as they provide:<ul> <li>Comprehensive Visibility: Maps the complex relationships between various cloud-native components (resources, identities, networking, security). This approach offers a holistic view of the cloud environment and helps in developing an architectural understanding of the various applications and workloads.</li> <li>Contextual Risk Prioritization: Different types of findings (vulnerabilities, misconfigurations) on a given resource, coupled with the potential attack paths for that resource (by analyzing IAM and networking) can be correlated to apply better risk prioritization and perform targeted remediations.</li> <li>Multi-Cloud Consistency: A unified graph data model where the representation of different resource types and relationships are standardized, can enable higher consistency in how security is applied across multi-cloud environments.   </li> </ul> </li> <li>Agentless Vulnerability Scanning: Due to the ephemeral nature of resources and the decentralized nature of operating in the cloud where application and development teams own their entire stacks (including the lifecycle of the VM instances), consistently deploying agents on VMs is challenging, especially at organizations where the concept of golden images doesn\u2019t exist. This is where agentless scanning can help streamline operations and provide automated vulnerability scanning capabilities.<ul> <li>However, agent-based solutions should be considered when CWPP is a priority as it is more effective in real-time monitoring / blocking of malicious activities / behavior.        </li> </ul> </li> <li>Prioritize Comprehensive APIs: Prefer vendors with well-designed and flexible APIs that do not limit you to carrying out actions only via their console. Robust APIs will enable you to automate processes and workflows, integrate with enterprise tools (such as ticketing and messaging systems, vulnerability repositories, and SIEMs), and efficiently handle reporting and remediation tasks.</li> <li>Alignment With Vendor Roadmaps: Assess the vendor's roadmap to ensure it aligns with your future requirements and strategic goals. For example, if your organization is making a big push for AI workloads in the near-future, consider vendors that have AI Posture Management (AIPM) capabilities in their short-term roadmap. </li> </ul> <p>By selecting a best-of-suite CNAPP solution that has CSPM, vulnerability scanning, and CIEM as its core capability and also has the required flexibility / extensibility as a platform, organizations can benefit from operational efficiencies, spend less time upskilling on multiple tools, and ultimately enhance their ability to manage risks in dynamic cloud-native environments.</p>"},{"location":"#day-1-focus-areas","title":"Day 1 Focus Areas","text":"<p>As CNAPP definitions evolve, new capabilities (read: acronyms) are added, and vendors market their platforms as the silver bullet for addressing all cloud security needs, it is tempting to enable every available feature. However, this approach can overwhelm security teams with alerts and frustrate developers with a sudden influx of issues that need to be remediated, creating friction between teams meant to collaborate on reducing security risks. To avoid these pitfalls, consider the following:</p> <ul> <li>Prioritize CSPM Operationalization: Prioritize enablement of CSPM and vulnerability scanning (from hereon, \u201cCSPM\u201d) on day one of your CNAPP implementation journey. This approach provides the following benefits:<ul> <li>Quick Value Realization: These findings are typically easier to understand and address, allowing you to establish a baseline understanding of your cloud security posture and conduct burndown campaigns to reduce risks and demonstrate \"quick-wins\". This justifies the benefits in your CNAPP investment, crucial for building momentum and securing stakeholder buy-in.</li> <li>Platform Familiarity: Starting with CSPM allows your security team to become familiar with the CNAPP tool's interface, graph database, data model, reporting mechanisms, and overall functionality. This foundational knowledge is essential for successfully implementing more advanced capabilities in the future.</li> <li>Upskilling Opportunity: Implementing CSPM first gives your security teams time to upskill on cloud security concepts, creating a foundation for future implementation of more advanced capabilities.</li> <li>Developer Acclimation: CSPM provides an opportunity for developers to understand how security issues are reported and how they need to consume and act on this information. This gradual introduction to the platform helps smooth the transition to a more security-aware development process.  </li> </ul> </li> <li>Enable Self-Service Consumption of Findings: Set up role-based access control (RBAC) to enable self-service consumption of security findings for the developer community. This approach is crucial for conducting timely remediation and fostering a culture of shared responsibility within the organization. Below are the benefits of enabling self-service:<ul> <li>Developer Empowerment: The onus for remediating issues is on the developers and cloud engineers; hence, providing them with self-service access to relevant security findings and remediation guidance allows them to take ownership of security issues associated with their applications.</li> <li>Accelerated Remediation: Generally, CNAPP tools have descriptive remediation guidance that is actionable. By putting information directly in the hands of those who can act on it, your organization can significantly speed up the fix process and reduce the overall time-to-remediation.</li> <li>Shared Responsibility: By providing developers with tool access, they can monitor the security posture of their applications on-demand, and provide inputs on false positives or potential policy adjustments, thereby fostering a collaborative culture for cloud security. Additionally, I have observed scenarios where developers and engineers often want to use CNAPP for their own non-security related use-cases (e.g., tagging, building cloud asset inventory, identifying resources that are no longer needed, etc.), thereby further making the case for the platform.  </li> </ul> </li> <li>Metrics &amp; Reporting: Develop reporting for CSPM, ideally aligned to your SLA / vulnerability management standards, to measure the overall health of the program and remediation effectiveness. This will enable cultural changes and set proper expectations around remediation timelines.  </li> <li>Conduct Regular Office Hours: Hold regular sessions and office hours, participate in developer forums, set up communication channels, etc. to educate the developers on the platform and hand-hold them through remediations when doing the initial roll-out. This will build goodwill and allow you to develop effective processes that incorporate valuable insights based on how your development teams think and operate.</li> <li>Custom Develop &amp; Fine-Tune CSPM Policies: As your organization becomes familiar with the initial roll-out, it is crucial to move beyond out-of-the-box (OOB) policies towards custom policy development and fine-tuning (\u201ccustomization\u201d) to better align with the organization\u2019s security standards and reduce false positives. Below are key considerations for customizing policies:<ul> <li>Reduce Alert Fatigue &amp; False Positives: Out-of-the-box policies often generate a high volume of alerts, many of which may be false positives / irrelevant to your environment. Customization helps reduce noise and allows your teams to focus on the most critical issues.</li> <li>Align with Security Standards: OOB policies may not adequately address your organization's unique risk profile or compliance requirements and may not be aligned with your security standards and controls. By developing custom policies based on security standards, cloud usage patterns and application architectures, you can have higher confidence in your policy set and improved accuracy in the legitimacy of the issues identified. Additionally, since these policies are contextualized for your organization\u2019s cloud environments, they reduce friction and improve adoption among your development teams.</li> </ul> </li> </ul> <p>Prioritizing CSPM as your initial focus establishes a robust foundation that not only addresses critical security risks but also allows the organization to build confidence in its processes, workflows, and ability to remediate issues. This provides the foundation for the implementation of more advanced CNAPP capabilities in a manageable manner with minimal resistance / disruptions.</p>"},{"location":"#closing-thoughts","title":"Closing Thoughts","text":"<p>In summary, the tool in itself is very powerful and provides good visibility but the successful implementation and ROI are dependent on people and process elements where activities like fostering a shared responsibility culture, partnering with and enabling developers, phasing the roll-out of the platform, and designing effective processes are critical factors for success. </p> <p>PS: CNAPP is a broad area with many layers to unpack and there are several topic areas (listed below) that I did not cover in this blog post that require deep-dive of their own and will be a good candidate for a follow-up blog on \u201cday 2\u201d activities and beyond:</p> <ul> <li>Pre-deployment security capabilities such as container image scanning, secrets scanning, software supply chain  </li> <li>Integrations with ticketing and messaging platforms (Jira, Slack), SIEM (Splunk, Sentinel), vulnerability warehouse (ServiceNow VR)  </li> <li>Strategy for routing different styles of issues to different sources  </li> <li>Operationalizing the various CNAPP capabilities </li> </ul> <p>References:</p> <ol> <li>Gartner - https://www.gartner.com/doc/reprints?id=1-2I6V52K2&amp;ct=240726&amp;st=sb</li> </ol>"},{"location":"AWS%20Security%20Guardrails%20%26%20Terraform/","title":"AWS Security Guardrails &amp; Terraform","text":""},{"location":"AWS%20Security%20Guardrails%20%26%20Terraform/#introduction","title":"Introduction","text":"<p>Traditional security approaches designed for on-premises environments are inadequate in addressing the unique risks posed by the cloud. The distributed and dynamic nature of cloud resources, speed of development and innovation, multi-tenant architectures, and decentralized operating models have resulted in a complex threat landscape that requires a fundamental shift in building and operating scalable security programs. This paradigm shift has popularized the concepts of paved roads and security guardrails to reduce the security burdens on the engineers and enable them to focus on innovation and driving business value, without compromising on security. Below, I have included some helpful resources better to explain the concepts of paved roads and security guardrails but at a high-level:</p> <ul> <li>Paved Roads: Originally conceptualized by Netflix, refers to a set of standardized frameworks (e.g., authentication patterns, certificate management, service mesh, etc.), self-service tools, and automated processes that are readily consumable by engineers, allowing them to focus on their core responsibilities and not worry about implementing security requirements.  </li> <li>Security Guardrails: These are preventive controls, integrated into the development workflows, that define the security boundaries and force the engineers to operate with them, thereby stopping misconfigured and vulnerable resources (code) from being released into cloud environments.</li> </ul> <p>Most organizations today have adopted some form of paved roads or security guardrails, although the maturity of these implementations varies significantly. Broadly, the focus has been on two key areas: (a) scanning and detection early in the Software Development Life Cycle (SDLC) and (b) ad hoc prevention of high-risk actions through CI/CD pipeline integrations and organizational policies (e.g., Service Control Policies (SCPs), Organization Policies, or Azure Policies). These efforts prioritize proactive identification of security issues early in the development process to help catch and address them before they manifest within cloud environments.</p> <p>Additionally, for many organizations operating in the cloud, implementing Cloud-Native Application Protection Platform (CNAPP) has become essential for achieving greater visibility into cloud environments and identifying diverse classes of security issues (I recently wrote blog posts on Day 1 and Day 2 focus areas for CNAPP).</p> <p>While conducting scans and generating insights is a foundational first step toward understanding and securing the environment, a critical gap remains. Organizations often lack prescriptive guidance and accessible, reusable security artifacts that engineers can leverage directly. By not providing these resources, security teams inadvertently place the burden of configuring services securely on engineers, who may lack the security expertise or resources needed to efficiently address intricate security requirements. Without \u201cbuilt-in\u201d security, engineers end up spending time on tasks that could otherwise be streamlined through reusable secure Infrastructure as Code (IaC) templates (It is best practice to leverage IaC for cloud infrastructure provisioning and management). This kind of \u201csecure-by-design\u201d IaC allows engineers to meet security requirements faster, enabling them to stay focused on their primary responsibilities.</p> <p>Establishing such reusable, secure IaC offers additional benefits beyond efficiency. By actively supporting engineers with secure IaC, security teams move from a passive, reactive role into a proactive partnership, where they provide engineers with actionable solutions rather than just requirements. This approach fosters stronger relationships with development teams, as security becomes an enabler rather than a gatekeeper, actively facilitating good security practices instead of merely imposing them.</p> <p>The CNAPP tools (outside of IaC scanning / policy-as-code (PaC)) are great at scanning and flagging issues and offering remediation guidance that is generally applicable for CLI or console users. However, this guidance falls short for organizations heavily invested in IaC, as engineers must still research and implement the specific IaC parameters needed to address the issues. This gap highlights the need for security teams to go beyond issue reporting. By building and distributing \u201cbuild once, use multiple times\u201d secure IaC templates, organizations can streamline remediation efforts, ultimately leading to faster, more scalable, and more consistent security practices.</p> <p>In summary, organizations invest significant resources in scanning and monitoring to ensure they have visibility into their security posture. Yet, a lack of centralized, reusable security artifacts often leaves engineers to interpret and implement security requirements on their own. By investing in reusable secure IaC, security teams can enable engineers to \u201cinherit\u201d security best practices, rather than requiring them to navigate complex configurations alone. This approach not only enhances efficiency but also builds a more collaborative relationship between security and engineering, reinforcing security as a shared responsibility across the organization.</p>"},{"location":"AWS%20Security%20Guardrails%20%26%20Terraform/#solution-overview","title":"Solution Overview","text":"<p>Given the benefits of secure IaC templates with built-in security requirements, below is a high-level overview of automation (source code here) that leverages Artificial Intelligence (AI) to develop:</p> <ul> <li>List of security requirements for several commonly used AWS services  </li> <li>Secure Terraform modules that codify these security requirements</li> </ul> <p></p>"},{"location":"AWS%20Security%20Guardrails%20%26%20Terraform/#requirements-generator","title":"Requirements Generator","text":"<p>The Requirements Generator (requirements-generator.py) is a Python script that consolidates and enhances AWS service security requirements from multiple scanning tools (Checkov and Prowler). The script leverages Anthropic\u2019s Claude 3.5 Sonnet model via AWS Bedrock and through prompt engineering, aims to transform the security requirements from these distinct sources into comprehensive, well-structured guidelines that engineers can easily understand and implement. Below are the key features that I have tried to implement:</p> <ul> <li>Multi-Source Integration: Combines security requirements from multiple sources (Prowler and Checkov) into a unified format  </li> <li>Intelligent Deduplication: Merges duplicate requirements while maintaining distinct requirements for different resources  </li> <li>Environment-Aware Processing: Considers specific AWS environment configurations like hub-and-spoke network architecture and SSO setup  </li> <li>Standardized Output: Generates consistently formatted JSON output with required fields like ID, name, description, cloud provider, and security domain  </li> <li>Comprehensive Coverage: Identifies gaps in security requirements and adds missing controls based on AWS best practices</li> </ul> <p>Below is the large language model (LLM) prompt that I am currently using for requirements generation:</p> <pre><code>You are a cloud security expert that is tasked with defining detailed technical security requirements AWS services. \nThe security requirements should be clear, well-worded, and descriptive with all the necessary details such that it can be easily understood and implemented by developers.\n\n  Follow the below guidelines when developing technical security requirements:\n  1. Below are the key details about my AWS environment's setup that needs to be adhered to always:\n      - Network is setup to follow a hub and spoke architecture where the VPCs are connected to each other via Transit Gateways.\n      - AWS management console access is federated through SSO via AWS Identity Center. IAM users are only created as an exception.  \n      - AWS IAM user access keys are banned from use and instead IAM roles should be used.\n      - CloudTrail management events have been enabled for all my AWS accounts for all services.\n      - VPC flow logs have been enabled for all my VPCs for all AWS accounts. \n      - You can assume that all the resources are only private and have no requirement for any sort of public access.\n      - Any resources that need to be publicly exposed to the internet, are managed separately and is outside the scope of the requirements defined here. \n\n  2. You will be provided security requirements enclosed within the &lt;security requirements&gt; tag for different AWS services as a starting point. \n      - If there are duplicative requirements on the same resource, combine them.\n      - If the security violates / do not follow my environment's specific setup, remove them. \n      - if there are 2 requirements on the same resource for the same configuration, combine them into 1.\n          - For example: If there are 2 requirements for encryption-at-rest on sagemaker notebook, combine them into 1.\n      - If there are 2 requirements on 2 different resources for the same configuration, keep them separate. \n          - For example: If there is 1 requirement for encryption-at-rest on sagemaker notebook and another 1 for encryption-at-rest on sagemaker domain, keep them as separate.\n\n  3. Add any missing security requirements for the given service and all of its resources to ensure a robust and comprehensive library of security requirements.          \n     - Examples of security requirements that should be added if missing: use latest TLS policies, enforce HTTPS, disable public access, enable encryption at-rest using CMK, etc.\n\n  4. When writing the \"name\" and \"description\", make it very clear and well-worded such that it is easy to understand and can be easily implemented by developers.\n     - The \"name\" of the requirement should be a proper sentence and should be easy to understand. \n     - The \"description\" of the requirement should be detailed and should contain implementation details for that requirement. \n     - Pack all the details and context in the \"name\" and \"description\" of the requirement to make it easy to understand and implement for developers.\n     - Don't abstract any details by using generic phrases like \"apply secure settings\". Instead, enumerate each setting that needs to be applied - \"enabling TLS, enabling audit logging, enabling user activity logging, etc.\"  \n     - Don't develop security requirements that are vague or generic.\n     - The granularity / specificity of these security requirements should be such that it can be interpreted by developers and translated into IaC easily.\n\n  5. When defining security requirements for the AWS service, only focus on requirements for that service. Don't define requirements for other services.\n     - For example:\n          - Avoid requirements like \"Implement Regular Key Rotation for AppFlow KMS Keys\" because this is captured in the requirements for AWS KMS by an overarching requirement like \"Customer managed keys (CMK) should have rotation enabled for every 90 days\".\n          - Within AppFlow, we should only call out a requirement like \"Use AWS KMS Customer Managed Key (CMK) for AppFlow Flow Encryption\".\n\n  6. Perform a detailed review on the developed security requirements to make sure the guidelines are followed and the below hold true:            \n      - Requirements like \"Enable AWS CloudTrail Logging for MSK Cluster API Calls\" \"Enable AWS CloudTrail Logging for AppFlow API Calls\" don't exist because CloudTrail logs are already enabled for all services.\n      - Requirements like \"\"Enable VPC Flow Logs for MWAA VPC\" don't exist because VPC flow logs have been enabled for my VPCs. \n      - Requirements like \"Implement Multi-Factor Authentication for Critical Route53 Changes\" don't exist because federation / SSO through Identity Center enforces MFA.\n      - Requirements with generic phrases like \"restrict access\" or \"enforce principal of least privilege\" don't exist. Instead, they are written more clearly like \"restrict access to only known principals or accounts\".\n      - Vague requirements that are subject to interpretation don't exist.\n          - For example: Requirements like \"Ensure Proper Configuration of Elastic Load Balancer Listeners\" and \"Implement Secure Listener Rules for Application Load Balancers\" don't exist. These are bad requirements because it is not clear how to implement this and is subject to interpretation by the developers. Instead, these requirements should be written where the exact configurations (HTTPS and latest TLS policies) are specified very clearly to ensure that all the necessary details are provided to implement secure load balancer listeners.       \n      - Requirements are defined on all the different resources for the AWS service. \n          - For example: If there are requirements only for sagemaker domain but not sagemaker notebook, develop requirements for sagemaker notebook. Keep the requirements for sagemaker domain and sagemaker notebook separate, don't combine them.\n      - Requirements where different types of configurations are combined don't exist. Instead, keep them as 2 separate requirements.\n          - For example: \"Enable encryption at rest and in transit for MSK Cluster\" is a bad requirement. These should be split into 2 separate requirements like \"Use KMS CMK for encryption-at-rest for MSK Cluster\" and \"Use latest TLS policies for encryption-in-transit for MSK Cluster\".\n      - Requirements where different types of resources are combined don't exist. Instead, keep them as 2 separate requirements. \n          - For example:\n              - \"Use KMS CMK for encryption-at-rest for SageMaker Domain and Notebook Instance\" is a bad requirement. These should be split into 2 separate requirements like \"Use KMS CMK for encryption-at-rest for SageMaker Domain\" and \"Use KMS CMK for encryption-at-rest for SageMaker Notebook Instance\"\n              - \"Use Non-Default Ports for RDS Instances and Clusters\" is a bad requirement. These should be split into 2 separate requirements like \"Use Non-Default Ports for RDS Instances\" and \"Use Non-Default Ports for RDS Clusters\"\n\n  7. Update requirement IDs to follow format 'service:001', 'service:002', etc.\n      - For example: s3:001; s3:002; s3:003, etc.\n\n  8. Assign a \"domain\" to each requirement based on \"name\" and \"description\".\n     - Possible values:\n          - data protection\n          - network security\n          - identity and access management\n          - logging and monitoring\n          - secure configuration\n\n  9. Ensure each requirement has these fields in this exact order:\n     - ID (format: service:00X)\n     - name (brief title)\n     - description (detailed explanation)\n     - cloudProvider (always \"AWS\")\n     - domain\n\nRespond with ONLY a valid JSON array of requirements. Do not include any explanatory text or markdown formatting.\n</code></pre>"},{"location":"AWS%20Security%20Guardrails%20%26%20Terraform/#terraform-creator","title":"Terraform Creator","text":"<p>The Terraform Creator (terraform-creator.py) is a Python script that automatically generates secure Terraform modules based on the security requirements that were developed by the \u201cRequirements Generator\u201d script. The script utilizes Anthropic's Claude 3.5 Sonnet model via AWS Bedrock and employs prompt engineering to transform security requirements into reusable and secure Terraform modules. Below are the key features that I have attempted to implement:</p> <ul> <li> <p>Standardized Module Structure: Generates three files for each AWS service:</p> <ul> <li>main.tf: Contains resource configurations with requirement traceability  </li> <li>variables.tf: Defines all configurable parameters with secure default  </li> <li>notes.md: Provides detailed implementation documentation and coverage analysis</li> </ul> </li> <li> <p>Intelligent Implementation Decisions: Makes informed choices about requirement implementation:</p> <ul> <li>Assumes reusability where users will bring pre-created resource components (KMS keys, log buckets, etc.) as opposed to creating supporting resources (see below example) </li> <li>Avoids creating duplicate resources for implementing different requirements  </li> <li>Creates optional read/write IAM policies for flexibility</li> </ul> </li> <li> <p>Comprehensive Documentation: Maintains detailed documentation of implementation status:</p> <ul> <li>Maps security requirements to specific Terraform configurations  </li> <li>Documents partially implemented requirements  </li> <li>Explains requirements that cannot be implemented via Terraform  </li> <li>Includes additional security measures beyond base requirements</li> </ul> </li> </ul> <p>Below is the LLM prompt that I am currently using for Terraform creation:</p> <pre><code>You are an expert in cloud security for AWS. You need to develop comprehensive, secure Terraform module for the given AWS service based on the\nsecurity requirements enclosed within the &lt;security requirements&gt; tag. Follow the below guidelines when developing secure Terraform modules:     \n\n  1. The attempt should be to develop secure Terraform modules for all the security requirements for the given AWS service.\n\n  2. Use the \"name\" and \"description\" fields to develop understanding of the requirement for Terraform implementation.\n\n  3. If there are any requirements that cannot be implemented in Terraform or can only be implemented partially, do so and maintain a note in the notes.md file.\n      - Don't implement requirements for VPC endpoints like \"Implement VPC Endpoints for Kinesis Data Streams\" because VPC endpoints are managed outside of this module.\n      - Don't implement requirements like \"Enable AWS CloudTrail Logging for AppFlow API Calls\" because CloudTrail logging has already been enabled for all accounts and all services.\n      - Since IAM policies are use-case dependent, create 1 policy for read and 1 policy for write that users can OPTIONALLY use.\n      - For the same resource, don't create multiple separate resource blocks. Instead, create 1 resource block with all the configurations. \n          - For example:\n              - Instead of creating 2 different resources like \"resource \"aws_kms_key\" \"main\"\" and \"resource \"aws_kms_key\" \"tagged\"\", create only 1 \"aws_kms_key\" with all the configurations.\n              - Instead of creating 2 different resources like \"resource \"aws_apigatewayv2_stage\" \"xray\"\" and \"resource \"aws_apigatewayv2_stage\" \"main\"\", create only 1 \"aws_apigatewayv2_stage\" with all the configurations.\n\n  4. DO NOT create supporting resources like s3 buckets, security groups, kms keys, cloudwatch log groups / alarms, etc. Assume that those have been created and the values will be provided.\n      - For example: \n          - When enabling access logs for ALB, assume that the s3 logs destination bucket is already created and value will be provided by user as input.\n          - When setting up security groups for resources (RDS, ECS, EKS, etc.), assume that the security group has already been created and value will be provided by user as input.\n          - When setting up encryption on a resource, assume that the KMS key has already been created and value will be provided by user as input.              \n\n  5. Even when not specified explicitly, always implement security best practices like encryption at rest for all possible resources, latest TLS policies, disabling insecure defaults (e.g. disable_execute_api_endpoint = true for aws_api_gateway_rest_api), etc.\n\n  6. Below are the 3 files that should be created:\n      a. main.tf: Include all necessary resources and their configurations. Add comments for each requirement that was implemented in Terraform in the format ID:name directly above the actual configuration.\n      b. variables.tf: Define all variables used in main.tf, including descriptions and default values where appropriate.\n          1. Set the default values to be the most secure values possible. E.g., set ALB to internal = true instead of default value of internal = false.\n      c. notes.md: Provide a detailed breakdown in markdown syntax such that all requirements for the given AWS service are individually accounted for:\n          1. Requirements that were implemented in the Terraform code. Ensure you implement requirements in Terraform that are partially possible and maintain a note on that. \n          2. Requirements that could be implemented but weren't included (if any) - this list should be as minimal as everything possible to implement through Terraform must be implemented.\n          3. Requirements that are inherently implemented by the implementation of a different requirement.\n          4. Requirements that cannot be directly implemented in Terraform along with an explanation.  \n          5. Any best practices or additional security measures not mentioned in the requirements but relevant to the AWS service.\n          6. Each of the requirements for the given AWS should be its own individual line, don't merge requirements like S3-001, S3-002, etc. in 1 row.\n          7. Example format for notes.md is provided below within the &lt;notes.md example&gt; tag.\n\n  7. Review all the 3 files generated to make sure that they are accurate, align to the provided security requirements for the AWS service, and follow all the rules specified above.\n\nEnsure that the Terraform code is well-commented, follows best practices, and is as secure as possible. Include input variables for all configurable parameters to make the module reusable.\n\nFormat your response exactly as below within the XML tags and do not include anything else:\n\n&lt;main.tf&gt;\nmain.tf content\n&lt;/main.tf&gt;\n\n&lt;variables.tf&gt;\nvariables.tf content\n&lt;/variables.tf&gt;\n\n&lt;notes.md&gt;\nnotes.md content\n&lt;/notes.md&gt;\n</code></pre> <p> <p>Below is an illustrative example to highlight how Terraform modules can be reused:</p> <p></p>"},{"location":"AWS%20Security%20Guardrails%20%26%20Terraform/#closing-thoughts","title":"Closing Thoughts","text":"<p>While this implementation primarily focused on developing security requirements and Terraform modules for AWS, it provides a flexible framework that can be customized for other IaC languages, cloud providers, and security requirements. By updating the prompts for the LLM and adjusting configurations\u2014such as integrating Prowler and Checkov\u2019s policies for Azure or GCP, incorporating security requirements from alternative tools, or using your own requirements\u2014this automation can be adapted to suit various use-cases.</p> <p>With the help of AI, I was able to create these requirements and modules in a few hours with reasonable accuracy, which would otherwise have taken me weeks or even months. However, since the requirements and modules are AI-generated, it\u2019s important to use them with caution; I recommend reviewing, validating, and tailoring the content to meet your specific needs. Rather than viewing this as a silver bullet, I see it as an enabler for enhancing efficiency in developing secure IaC. Overall, I am optimistic about its potential benefits:</p> <ul> <li>Time Savings for Engineers: They can avoid starting from scratch when implementing security configurations.  </li> <li>Support for Security Teams: AI can help alleviate the burden of developing remediation guidance, allowing teams to focus on more complex issues.</li> </ul> <p>While this implementation is designed to generate secure IaC starter packs, it illustrates a broader point: AI solutions can offer much-needed support to overworked security teams. As models evolve and improve, the quality of AI-generated outputs will only get better, enabling us to tackle more complex security challenges efficiently.</p> <p>Resources for paved roads and security guardrails:</p> <ol> <li>Netflix\u2019s talk at RSA - Construction Time Again: A Lesson in Paving Paths for Security </li> <li>Google Cloud - Building security guardrails for developers </li> <li>Resourcely\u2019s conversation with Jason Chan - Guardrails and Paved Roads </li> <li>Jason Chan\u2019s talk at LASCON: From Gates to Guardrails - Alternate Approaches to Product Security </li> <li>Netflix\u2019s blog post - The Show Must Go On </li> <li> <p>Clint Gibler\u2019s chats:</p> <ol> <li>Jason Chan on the Origins of the Paved Road</li> <li>Netflix\u2019s Scott Behrens on the Difficulty of Building a Useful Paved Road</li> </ol> </li> <li> <p>Resourcely\u2019s open-source project - Cloud Guardrails</p> </li> </ol>"},{"location":"Deep%20Dive%20-%20AWS%20Organization%20Policies%20%28Part%201%29/","title":"Deep Dive: AWS Organization Policies (Part 1)","text":""},{"location":"Deep%20Dive%20-%20AWS%20Organization%20Policies%20%28Part%201%29/#introduction","title":"Introduction","text":"<p>As organizations scale their cloud infrastructure, managing AWS accounts securely and efficiently becomes both a necessity and a challenge. Today, companies heavily rely on AWS Organizations or AWS Control Tower to manage their multi-account AWS environments and meet their business, governance, security, and operational goal. This strategy enables workload isolation, improved quota and resource management, and the enforcement of security controls across environments (development, testing, and production). However, managing AWS accounts at scale introduces complexities \u2014 maintaining governance, enforcing guardrails, and streamlining operations across the organization.</p> <p>This is where AWS Organization Policies play a pivotal role. These are rules that are applied at the organizational level for controlling resource access, enforcing security controls, and ensuring standardized configurations across multiple AWS accounts. They ensure that all accounts within an organization operate within these defined boundaries, thereby balancing flexibility with proper governance. There are 2 types of AWS Organization Policies:</p> <ul> <li> <p>Authorization Policies: Authorization policies provide the ability to centrally define and enforce the maximum available permissions for principals and resources within your AWS Organizations. The two types of Authorization Policies are:  </p> <ul> <li>Service Control Policies (SCPs): SCPs allow you to centrally define and enforce maximum available permissions for principals (IAM users, root users, and roles) within your AWS Organizations.  </li> <li>Resource Control Policies (RCPs): RCPs allow you to centrally define and enforce the maximum available permissions for resources within your AWS Organizations.  </li> </ul> </li> <li> <p>Management Policies: Management policies provide the ability to centrally define and enforce configurations on services and resources within your AWS Organizations. The different types of Management Policies are:  </p> <ul> <li>Declarative Policies: Declarative policies allow you to centrally define and enforce baseline configuration of resources within your AWS Organizations.  </li> <li>Backup Policies: Backup policies allow you to centrally manage backups for resources within your AWS Organizations.  </li> <li>Tag Policies: Tag policies allow you to centrally enforce tagging standards on resources within your AWS Organizations.  </li> <li>Chatbot Policies: Chatbot policies allow you to centrally restrict access to resources within your AWS Organizations, from Teams, Slack, etc.  </li> <li>AI Services Opt-Out Policies: AI policies allow you to centrally control access to your data and prevent them from being used in the development of AWS\u2019 AI services.</li> </ul> </li> </ul> <p>In the remainder of this blog (Part 1), I will take a deep-dive into the two types of Authorization Policies: SCPs and RCPs. I will follow this with a subsequent blog (Part 2) that delves into the various types of Management Policies.</p>"},{"location":"Deep%20Dive%20-%20AWS%20Organization%20Policies%20%28Part%201%29/#service-control-policies-scps","title":"Service Control Policies (SCPs)","text":"<p>Service Control Policies (SCPs) are a type of authorization policy that provides you with centralized control over the maximum permissions that are available to the principals (IAM users, root users, roles) within your AWS Organization. By design, SCPs restrict permissions rather than grant them. Thus, they create permission guardrails and ensure that principals within your organizations operate within these predefined access boundaries. Below are key considerations when implementing SCPs:</p>"},{"location":"Deep%20Dive%20-%20AWS%20Organization%20Policies%20%28Part%201%29/#scp-applicability-scope","title":"SCP Applicability Scope","text":"<ul> <li>SCPs apply only to IAM principals managed by member accounts within your organization. They do not apply to IAM principals that reside outside your organization.</li> <li>SCPs do not apply to policies attached directly to resources (i.e., resource policies).  <ul> <li>For example, if an Amazon S3 bucket owned by account A has a bucket policy granting access to users in account B (outside the organization), the SCP attached to account A does not apply to those external users or the resource policies.</li> </ul> </li> <li>SCPs do not apply to service-linked roles.</li> <li>SCPs do not apply to IAM principals within the management account. However, they do apply to IAM principals within delegated admin accounts.</li> <li>SCPs do not apply to the following tasks/entities:<ul> <li>Register for the Enterprise support plan as the root user.</li> <li>Provide trusted signer functionality for CloudFront private content.</li> <li>Configure reverse DNS for an Amazon Lightsail email server and Amazon EC2 instance as the root user.</li> <li>Tasks on some AWS-related services:<ul> <li>Alexa Top Sites.</li> <li>Alexa Web Information Service.</li> <li>Amazon Mechanical Turk.</li> <li>Amazon Product Marketing API.</li> </ul> </li> </ul> </li> </ul>"},{"location":"Deep%20Dive%20-%20AWS%20Organization%20Policies%20%28Part%201%29/#scp-permission-evaluation-logic","title":"SCP Permission Evaluation Logic","text":"<ul> <li>SCPs operate on a deny-by-default model. If an action or service is not explicitly allowed by the SCP, it is implicitly denied, regardless of IAM permissions.<ul> <li>Hence, when SCPs are initially enabled, AWS attaches the <code>FullAWSAccess</code> policy at the root level of your organization. This ensures that all services and actions remain initially allowed until more restrictive policies are applied.</li> </ul> </li> <li>The permissions available to principals within accounts are restricted by the SCPs applied at every level above it in the organization. If a specific permission is denied or not explicitly allowed at the parent level (root, OU, or the principal\u2019s account), the action cannot be performed by the principal even if they have admin access.</li> <li>SCPs do not grant permissions; hence, IAM principals need to be assigned permissions explicitly via IAM policies.<ul> <li>Example: If access to a service (e.g., S3) is \u201cAllowed\u201d via SCPs but the principal does not have permissions assigned to it explicitly via IAM policies, the principal cannot access S3.</li> </ul> </li> <li>If an IAM principal has an IAM policy that grants access to an action:<ul> <li>And the SCP also explicitly allows the action, then the principal can perform that action.</li> <li>But if the SCP does not explicitly allow or deny the action, the principal cannot perform that action.</li> </ul> </li> <li>If permissions boundaries are present, access must be allowed by all three mechanisms \u2014 SCPs, permission boundaries, and IAM policies - to perform the action.</li> </ul> <p>The flowchart below provides a high-level overview of how access decisions are made when SCPs are enabled:</p> <p></p>"},{"location":"Deep%20Dive%20-%20AWS%20Organization%20Policies%20%28Part%201%29/#scp-development-and-testing","title":"SCP Development and Testing","text":"<ul> <li>Use \"Deny\" statements to enforce baseline security controls that you want to apply across your entire organization.  <ul> <li>Example: Prevent member accounts from leaving your organization.</li> </ul> </li> </ul> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"DenyLeavingOrganization\",\n            \"Effect\": \"Deny\",\n            \"Action\": [\n                \"organizations:LeaveOrganization\"\n            ],\n            \"Resource\": \"*\"\n        }\n    ]\n}\n</code></pre> <ul> <li>Use \"Deny\" statements with conditions to manage exceptions or enforce certain specific controls.  <ul> <li>Example: Enforce the use of IMDSv2 for EC2 instances. </li> </ul> </li> </ul> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"DenyRunInstancesWithoutIMDSv2\",\n            \"Effect\": \"Deny\",\n            \"Action\": \"ec2:RunInstances\",\n            \"Resource\": \"*\",\n            \"Condition\": {\n                \"StringNotEquals\": {\n                    \"ec2:MetadataHttpTokens\": \"required\"\n                }\n            }\n        },\n        {\n            \"Sid\": \"DenyRunInstancesWithHighHopLimit\",\n            \"Effect\": \"Deny\",\n            \"Action\": \"ec2:RunInstances\",\n            \"Resource\": \"*\",\n            \"Condition\": {\n                \"NumericGreaterThan\": {\n                    \"ec2:MetadataHttpPutResponseHopLimit\": \"3\"\n                }\n            }\n        },\n        {\n            \"Sid\": \"DenyAllActionsForInsecureRoleDelivery\",\n            \"Effect\": \"Deny\",\n            \"Action\": \"*\",\n            \"Resource\": \"*\",\n            \"Condition\": {\n                \"NumericLessThan\": {\n                    \"ec2:RoleDelivery\": \"2.0\"\n                }\n            }\n        },\n        {\n            \"Sid\": \"DenyMetadataOptionsModificationForNonAdmins\",\n            \"Effect\": \"Deny\",\n            \"Action\": \"ec2:ModifyInstanceMetadataOptions\",\n            \"Resource\": \"*\",\n            \"Condition\": {\n                \"StringNotLike\": {\n                    \"aws:PrincipalARN\": \"arn:aws:iam::*:role/ec2-imds-admins\"\n                }\n            }\n        }\n    ]\n}\n</code></pre> <ul> <li>Example: Prevent high-risk roles from changes except when made by whitelisted admin roles.</li> </ul> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"DenyAccessWithException\",\n            \"Effect\": \"Deny\",\n            \"Action\": [\n                \"iam:AttachRolePolicy\",\n                \"iam:DeleteRole\",\n                \"iam:DeleteRolePermissionsBoundary\",\n                \"iam:DeleteRolePolicy\",\n                \"iam:DetachRolePolicy\",\n                \"iam:PutRolePermissionsBoundary\",\n                \"iam:PutRolePolicy\",\n                \"iam:UpdateAssumeRolePolicy\",\n                \"iam:UpdateRole\",\n                \"iam:UpdateRoleDescription\"\n            ],\n            \"Resource\": [\n                \"arn:aws:iam::*:role/&lt;role to protect from unauthorized changes&gt;\"\n            ],\n            \"Condition\": {\n                \"ArnNotLike\": {\n                    \"aws:PrincipalARN\": \"arn:aws:iam::*:role/&lt;approved admin that can make changes&gt;\"\n                }\n            }\n        }\n    ]\n}\n</code></pre> <ul> <li>By default, AWS applies the managed SCP <code>FullAWSAccess</code>, to all entities in the organization, which grants access to all services and actions. Be careful when removing this policy and not replacing it with another suitable policy (one that explicitly allows access to your desired list of services), as you can inadvertently end up locking yourself out.  <ul> <li>Example: Access should only be granted to approved services (S3, EC2, DynamoDB), and all other service access should be blocked. You can do this by applying the below SCP and removing the default <code>FullAWSAccess</code> policy.</li> </ul> </li> </ul> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"AllowApprovedServiceAccess\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:*\",\n                \"ec2:*\",\n                \"dynamodb:*\",\n                \"organizations:*\"\n            ],\n            \"Resource\": \"*\"\n        }\n    ]\n}\n</code></pre> <ul> <li>AWS currently does not have any features or mechanisms to run SCPs in audit-mode to monitor the behavior and ascertain that SCPs won\u2019t inadvertently cause disruptions.  <ul> <li>Leverage service last accessed data in IAM to determine which services are in use versus not and then use this insight to develop SCPs.  </li> <li>SCPs should be deployed to non-production accounts/OUs first to confirm they meet the requirements and are not causing disruptions. Once there\u2019s reasonable assurance around the behavior of SCPs, only then extend the scope to production accounts/OUs.</li> <li>Enable CloudTrail logging and query for access denied events where the failure reason is \u201cservice control policy.\u201d Analyze the log entries to determine that all the denied events are intended and by design, and they are not blocking legitimate actions.</li> <li>Never apply SCPs directly to the root OUs before thoroughly testing in lower/non-production accounts/OUs.          </li> </ul> </li> <li>The blog post from AWS - Get more out of service control policies in a multi-account environment - does a great job of walking through different approaches / recommendations for rolling out SCPs across multi-account environments, while staying within the limits and quotas - policy size of 5120 characters (including whitespaces), and 5 SCPs per entity (root, OUs, accounts). </li> </ul>"},{"location":"Deep%20Dive%20-%20AWS%20Organization%20Policies%20%28Part%201%29/#scp-reference-materials","title":"SCP Reference Materials","text":"<p>Documentation, Blog Posts, and Videos:</p> <ul> <li>AWS - Codify your best practices using service control policies: Part 1 </li> <li>AWS - Codify your best practices using service control policies: Part 2 </li> <li>AWS - How to use AWS Organizations to simplify security at enormous scale </li> <li>AWS - Identity Guide \u2013 Preventive controls with AWS Identity \u2013 SCPs </li> <li>AWS - Best Practices for AWS Organizations Service Control Policies in a Multi-Account Environment </li> <li>AWS - Control VPC sharing in an AWS multi-account setup with service control policies</li> <li>AWS re:Invent 2024 - Security invariants: From enterprise chaos to cloud order</li> <li>SummitRoute - AWS SCP Best Practices </li> <li>ScaleSec - Understanding AWS Service Control Policies </li> </ul> <p>Example Policies:</p> <ul> <li>AWS - SCPs included within AWS documentation </li> <li>AWS - GitHub repository containing example SCPs</li> <li>Vendor / Open Source Projects for SCPs:  <ul> <li>ScaleSec</li> <li>PrimeHarbor </li> <li>ASecureCloud </li> <li>CloudPosse </li> <li>Salesforce\u2019s Allowlister - Creates SCPs that only allow AWS services compliant with preferred compliance frameworks (e.g., PCI, HIPAA, HITRUST, FedRamp High, FedRamp Moderate).  </li> </ul> </li> </ul>"},{"location":"Deep%20Dive%20-%20AWS%20Organization%20Policies%20%28Part%201%29/#resource-control-policies-rcps","title":"Resource Control Policies (RCPs)","text":"<p>The introduction of Resource Control Policies (RCPs) by AWS addresses critical security challenges inherent in cloud environments. While SCPs effectively set permission boundaries for IAM principals within an organization, they do not govern resource-based policies. This limitation can lead to unintended / backdoor access if resource policies are misconfigured, as SCPs cannot restrict permissions granted through resource-based policies. Additionally, managing these resource policies individually across a sprawling infrastructure is complex and burdensome for security teams. RCPs mitigate this issue by enabling centralized enforcement of access controls directly on resources across all member accounts within an AWS Organization.</p> <p>RCPs are a type of authorization policy that provides you with centralized control over the maximum permissions that are available for the resources within your AWS Organization. By design, RCPs restrict permissions rather than grant them. Thus, they create permission guardrails and ensure that resources within AWS Organizations can only be accessed within these predefined access boundaries. Unlike SCPs, which are principal-centric, RCPs are resource-centric, focusing on controlling access to AWS resources. Below are key considerations when implementing RCPs:</p>"},{"location":"Deep%20Dive%20-%20AWS%20Organization%20Policies%20%28Part%201%29/#rcp-applicability-scope","title":"RCP Applicability Scope","text":"<ul> <li> <p>RCPs apply only to resources managed by member accounts within your organization. They do not apply to resources that reside outside your organization.  </p> <ul> <li>Example: If an IAM principal in your member account (Account A) is trying to access an Amazon S3 bucket in Account B, then the RCP attached to Account A does not apply to the S3 bucket in Account B.  </li> </ul> </li> <li> <p>Unlike SCPs, which only apply to IAM principals within your organization, RCPs apply to principals external to your organization when they try to access resources within your organization.  </p> <ul> <li>Example: If an IAM principal in an external account (Account B) is trying to access an Amazon S3 bucket in your member account (Account A), then the RCP attached to account A applies to the principal when trying to access the S3 bucket.  </li> </ul> </li> <li> <p>RCPs apply to the following AWS services:  </p> <ul> <li>Amazon S3  </li> <li>AWS Key Management Service (KMS)  <ul> <li>However, RCPs do not apply to AWS-managed KMS keys as those are managed and used by AWS services on your behalf.  </li> </ul> </li> <li>AWS Secrets Manager  </li> <li>Amazon SQS  </li> <li>AWS Security Token Service (STS)  </li> </ul> </li> <li> <p>RCPs do not apply to resources within the management account. However, they do apply to resources within delegated admin accounts.  </p> </li> <li> <p>RCPs cannot be used to restrict access to service-linked roles.</p> </li> </ul>"},{"location":"Deep%20Dive%20-%20AWS%20Organization%20Policies%20%28Part%201%29/#rcp-permission-evaluation-logic","title":"RCP Permission Evaluation Logic","text":"<ul> <li>By default, when RCPs are enabled, AWS applies a managed RCP, <code>RCPFullAWSAccess</code> to all entities (root, OUs, accounts) in the organization, which allow access to pass through RCPs and assure that all your existing IAM permissions continue to operate as they did until more restrictive policies are applied. This policy cannot be detached.</li> <li>The permissions for a resource are restricted by the RCPs applied at every level above it in the organization. If a specific permission is denied or not explicitly allowed at any parent level (root, OUs, or resource\u2019s account), the action cannot be performed on the resource, even if the resource owner attaches a resource policy that allows full access to the principal.  </li> <li>When a principal makes a request to access a resource within an account governed by an RCP, the RCP becomes part of the policy evaluation logic to determine whether the action is permitted. This applies regardless of whether the requesting principal belongs to the same organization or an external account.  </li> <li>Since RCPs do not grant permissions, IAM principals must still be explicitly granted access via IAM policies. If an IAM principal lacks appropriate IAM permissions, they cannot perform the actions, even if an RCP allows those actions on the resource.  </li> <li>If permissions boundaries are present, access must be allowed by all three mechanisms \u2014 RCPs, permission boundaries, and IAM policies - to perform the action.</li> </ul> <p>The flowchart below provides a high-level overview of how access decisions are made when RCPs are enabled: </p>"},{"location":"Deep%20Dive%20-%20AWS%20Organization%20Policies%20%28Part%201%29/#rcp-development-and-testing","title":"RCP Development and Testing","text":"<ul> <li>Use \u201cDeny\u201d statements to enforce baseline security controls that you want to apply across your entire organization.  <ul> <li>Example: Block resource access for principals external to the organization.</li> </ul> </li> </ul> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"EnforceOrgIdentities\",\n            \"Effect\": \"Deny\",\n            \"Principal\": \"*\",\n            \"Action\": [\n                \"s3:*\",\n                \"sqs:*\",\n                \"kms:*\",\n                \"secretsmanager:*\",\n                \"sts:AssumeRole\",\n                \"sts:DecodeAuthorizationMessage\",\n                \"sts:GetAccessKeyInfo\",\n                \"sts:GetFederationToken\",\n                \"sts:GetServiceBearerToken\",\n                \"sts:GetSessionToken\",\n                \"sts:SetContext\"\n            ],\n            \"Resource\": \"*\",\n            \"Condition\": {\n                \"StringNotEqualsIfExists\": {\n                    \"aws:PrincipalOrgID\": \"&lt;org-id&gt;\"\n                }\n            }\n        }\n    ]\n}\n</code></pre> <ul> <li>Use \u201cDeny\u201d statements with conditions to manage exceptions or enforce certain specific controls.  <ul> <li>Example: Only allow service actions that are made using secure transport protocol (HTTPS). </li> </ul> </li> </ul> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"EnforceSecureTransport\",\n            \"Effect\": \"Deny\",\n            \"Principal\": \"*\",\n            \"Action\": [\n                \"sts:*\",\n                \"s3:*\",\n                \"sqs:*\",\n                \"secretsmanager:*\",\n                \"kms:*\"\n            ],\n            \"Resource\": \"*\",\n            \"Condition\": {\n                \"BoolIfExists\": {\n                    \"aws:SecureTransport\": \"false\"\n                }\n            }\n        }\n    ]\n}\n</code></pre> <ul> <li>AWS currently does not have any features or mechanisms to run RCPs in audit-mode to monitor the behavior and ascertain that RCPs won\u2019t inadvertently cause disruptions.  <ul> <li>RCPs should be deployed to non-production accounts / OUs first to confirm they meet the requirements and are not causing disruptions. Only once there\u2019s reasonable assurance around the behavior of RCPs can the scope be extended to production accounts / OUs be extended.</li> <li>Enable CloudTrail logging and query for access denied events. Analyze the log entries to determine that all the denied events are intended and by design, and RCPs are not blocking legitimate actions.</li> <li>Never apply RCPs directly to the root OUs before testing in lower / non-production accounts / OUs.</li> </ul> </li> <li>Like SCPs, RCPs have the same quotas and limits - policy size of 5120 characters (including whitespaces), and 5 RCPs per entity (root, OUs, accounts). </li> </ul>"},{"location":"Deep%20Dive%20-%20AWS%20Organization%20Policies%20%28Part%201%29/#rcp-reference-materials","title":"RCP Reference Materials","text":"<p>Documentation, Blog Posts, and Videos:</p> <ul> <li>Introducing resource control policies (RCPs), a new type of authorization policy in AWS Organizations </li> <li> <p>AWS re:Invent 2024 - New governance capabilities for multi-account environments</p> </li> <li> <p>Wiz - How to use AWS Resource Control Policies</p> </li> </ul> <p>Example Policies:</p> <ul> <li>AWS - RCPs included within AWS documentation </li> <li>AWS - GitHub repository containing example RCPs</li> </ul>"},{"location":"Deep%20Dive%20-%20AWS%20Organization%20Policies%20%28Part%201%29/#access-evaluation-logic","title":"Access Evaluation Logic","text":"<p>The overall access evaluation logic that AWS applies to determine whether an action is allowed or not is much more complex than what is described above for RCPs and SCPs. The above visuals only walk through how these Authorization Policies function conceptually to help enforce access controls and security requirements. There are other types of policies as well in the flow (e.g., resource policies, session policies, IAM policies, etc.), that increase the complexity of how access is evaluated. The below flowchart from AWS is a comprehensive walkthrough of how access decisions are made:  </p> <p>Below is an example to demonstrate access evaluation when <code>Alice</code> (principal inside the organization) and <code>Bob</code> (principal outside the organization) attempt S3 actions (<code>s3:GetObject</code>, <code>s3:PutObject</code>, <code>s3:DeleteObject</code>, and <code>s3:ListObjects</code>) on the <code>example-bucket</code> under <code>Account B</code> (account in the same organization as Alice).</p>"},{"location":"Deep%20Dive%20-%20AWS%20Organization%20Policies%20%28Part%201%29/#policies-applied","title":"Policies Applied","text":"Hierarchy Level SCP Policies RCP Policies Description Root Allows <code>s3</code>, <code>ec2</code>, <code>dynamoDB</code>, <code>organizations</code> Default RCPFullAWSAccess: Allows all services Broad access at the root level. OU: Production Denies <code>s3:GetObject</code> Denies all actions unless: <ul><li>The principal belongs to the same organization as Alice</li><li>Or the request originates from Bob's account</li></ul> Restricts access unless these conditions are met. Account B Denies <code>s3:DeleteObject</code> Denies <code>s3:PutObject</code> Explicitly restricts <code>s3:DeleteObject</code> and <code>s3:PutObject</code>. Resource Policy Allows <code>s3:*</code> for all principals Resource-based policy allows full access to all S3 actions."},{"location":"Deep%20Dive%20-%20AWS%20Organization%20Policies%20%28Part%201%29/#evaluation-results","title":"Evaluation Results","text":"Action Alice (Inside Org) Bob (Outside Org) s3:GetObject Denied at OU SCP (explicit deny at OU SCP) Allowed (OU RCP condition satisfied, no other deny) s3:PutObject Denied at Account RCP (explicit deny at Account RCP) Denied at Account RCP (explicit deny at Account RCP) s3:DeleteObject Denied at Account SCP (explicit deny at Account SCP) Allowed (OU RCP condition satisfied, SCP skipped) s3:ListObjects Allowed (passes all checks and conditions) Allowed (OU RCP condition satisfied, no other deny) <p>Below is a visual walkthrough of the above scenario to showcase how access is evaluated to make \"Allow\" / \"Deny\" decisions: </p> <p></p>"},{"location":"Deep%20Dive%20-%20AWS%20Organization%20Policies%20%28Part%201%29/#data-perimeter","title":"Data Perimeter","text":"<p>When SCPs and RCPs are used together, they establish the foundational components for a data perimeter within your organization. At a high level, a data perimeter involves three key components\u2014 trusted identities, trusted resources, and expected networks \u2014 that work together to ensure that only whitelisted identities from known networks can access your organization\u2019s resources.</p> <ul> <li>Trusted Identities: IAM principals within the organization, explicitly trusted external accounts, or AWS on your behalf.  </li> <li>Trusted Resources: Resources within your organization, resources belonging to explicitly trusted external accounts, or resources that AWS uses on your behalf.  </li> <li>Expected Networks: Your VPCs, on-premise networks, or networks that AWS uses on your behalf.</li> </ul> <p>The diagram below from AWS provides a high-level overview of the concept of data perimeters:</p> <p></p> <p>By implementing only SCPs and RCPs, you will have an accelerated start on the journey of setting up a data perimeter. However, this alone will not give you a full setup that covers all services. For a robust implementation of a data perimeter, there are other key elements (and arguably the harder ones to implement), listed below, that also need to be in place:</p> <ul> <li>Resource Policies: Not all AWS services that support resource policies are also supported by RCPs (e.g., SNS, ECR, API Gateways). For these services, resource policies will still need to be applied in a decentralized manner on a per-resource basis, significantly increasing the complexity of extending the perimeter to these additional services.  </li> <li>VPC Endpoint Policies: To enforce that identities and resources are accessed from expected networks, AWS recommends using VPC endpoint policies. However, like resource policies, configuring and managing VPC endpoints at scale across all the VPCs in your organization for every supported AWS service is complex and requires significant effort.  <ul> <li>AWS\u2019s whitepaper on secure and scalable networking architecture includes a section on implementing centralized VPC endpoints in a hub-and-spoke model. The whitepaper can be found here.</li> </ul> </li> </ul> <p>The flowchart below outlines how the different policies, along with the requisite IAM condition keys, work together to achieve a secure data perimeter:</p> <p></p> <p>In conclusion, SCPs and RCPs are an important stride toward building a data perimeter that aligns trusted identities, trusted resources, and expected networks. However, progressing from here to a fully realized data perimeter is a strategic, multi-layered effort that must evolve in lockstep with the complexity of your AWS environments. Achieving this level of control involves deep insights into the inner workings of your AWS environment, including the identity models of each service, the metadata tags that guide resource governance, and the network paths \u2014 both on-premises and in the cloud \u2014 that support data flows. You must also know exactly which third parties interact with your systems and from which locations, and maintain visibility into how these relationships change over time.</p> <p>This effort involves incrementally expanding beyond the basics, starting with core AWS services and methodically layering on additional controls for other resources. Over time, it will also require the incorporation of resource policies, VPC endpoint policies, and other service-specific measures to tighten the perimeter. Additionally, a well-defined tagging strategy is essential as it enables consistent governance, supports automated guardrails, helps with exception management, and simplifies the application of policies across large, dynamic environments. </p> <p>Ultimately, implementing a robust data perimeter is a multi-year undertaking that requires time, operational discipline, and organizational buy-in. It relies on strong foundational elements such as granular identity controls, consistent tagging practices, well-managed exceptions, resource governance, and secure network setup. By taking a phased, service-by-service approach and continually refining your controls, you can evolve from a simple perimeter concept into a fully realized data perimeter that safeguards your organization\u2019s critical assets in a complex and ever-evolving AWS landscape.</p>"},{"location":"Deep%20Dive%20-%20AWS%20Organization%20Policies%20%28Part%201%29/#data-perimeter-reference-materials","title":"Data Perimeter Reference Materials","text":"<p>Documentation, Blog Posts, and Videos:</p> <ul> <li>AWS - Blog Post Series: Establishing a Data Perimeter on AWS </li> <li>AWS re:Inforce 2024 - Establishing a data perimeter on AWS, featuring Capital One (IAM305)</li> </ul> <p>Example Policies:</p> <ul> <li>AWS - GitHub repository containing example data perimeter policies</li> </ul>"},{"location":"Deep%20Dive%20-%20AWS%20Organization%20Policies%20%28Part%201%29/#closing-thoughts","title":"Closing Thoughts","text":"<p>Both SCPs and RCPs are integral for managing permissions and enforcing governance across multi-account AWS environments. While SCPs set permission guardrails for IAM principals, RCPs set permission guardrails for resources. In addition to defining maximum available permissions for principals and resources within your organization, SCPs and RCPs can also be used to enforce security controls (e.g., preventing users from uploading unencrypted S3 objects, enforcing IMDSv2 for EC2 instances, or requiring HTTPS connections to resources). Together, these policies provide a centralized capability to control access, enforce security requirements, and also lay the foundations for a well-defined data perimeter.</p> <p>This is part 1 of mult-part blog series where in the next blog(s), I will try to do a similar deep-dive into the different types of Management Policies.   </p>"},{"location":"Implementing%20CNAPP%3A%20Day%202%20Focus%20Areas/","title":"Implementing CNAPP: Day 2 Focus Areas","text":"<p>In the first part of the Cloud-Native Application Protection Platform (CNAPP) blog (here), I laid out my thoughts on tool selection criteria and day 1 focus areas. In this blog, I want to double-click into the day 2 focus areas that include pre-deployment security, implementing CNAPP capabilities beyond Cloud Security Posture Management (CSPM), designing issues management workflows, and integrating CNAPP with enterprise tools. Below is the high-level structure of how this document is organized:</p> <ol> <li>Pre-Deployment Security Capabilities </li> <li>Capabilities Beyond CSPM </li> <li>Operational Workflows and Technology Integrations</li> </ol>"},{"location":"Implementing%20CNAPP%3A%20Day%202%20Focus%20Areas/#pre-deployment-security-capabilities","title":"Pre-Deployment Security Capabilities","text":"<p>In the context of CNAPP, \"pre-deployment\" security encompasses a range of capabilities integrated into CI/CD pipelines and the overall development lifecycle to identify and block insecure resources from reaching cloud environments. Often, I have noticed that this term is narrowly defined to apply only to production environments. However, in my experience, a broader interpretation of this term\u2014viewing it as applicable to all stages before cloud deployment, i.e., \u201cbefore reaching cloud environments\u201d\u2014is more effective. This allows you to shift left and roll out capabilities in a standardized manner rather than managing disparate processes across different environments. Additionally, you can then implement policy thresholds that enforce stricter security requirements in higher environments compared to lower environments to provide the necessary freedom needed for experimentation, development, and testing. Below are the primary pre-deployment capabilities:</p> <ol> <li>Infrastructure-as-Code (IaC) and Secret Scanning </li> <li>Container Image Scanning and Software Composition Analysis (SCA)</li> </ol>"},{"location":"Implementing%20CNAPP%3A%20Day%202%20Focus%20Areas/#iac-and-secret-scanning","title":"IaC and Secret Scanning","text":"<p>This involves scanning resource deployment files\u2014such as Terraform, CloudFormation, ARM templates, Ansible playbooks, Kubernetes manifests, Helm charts, and Dockerfiles\u2014to identify misconfigurations and exposed secrets. Below are some key considerations for implementing IaC and secret scanning: </p> <ul> <li> <p>Scaling Across CI / CD Pipelines: In a cloud world where decentralized operating models are predominant, teams have autonomy over their development processes which leads to the proliferation of diverse pipelines, creating a complex landscape for security integration. This is because:</p> <ul> <li>Teams often utilize various CI/CD tools\u2014such as Jenkins, GitHub Actions, Azure DevOps, etc.\u2014each with distinct configurations and execution environments. Additionally, in large enterprises, the sheer volume of pipelines, often numbering in the hundreds or thousands, significantly increases the number of points that require security integration. This diversity complicates the enforcement of consistent scanning practices, as each pipeline may necessitate a customized approach to effectively implement IaC and secret scanning.</li> <li>Moreover, the decentralized nature of pipeline management means that security teams often lack visibility into all existing pipelines, making it challenging to ensure comprehensive coverage. The variety in pipeline configurations also complicates the task of standardizing scanning tools and processes, as what works for one pipeline might not be compatible with another.</li> </ul> <p>To address CI / CD scalability challenges, consider the following:</p> <ul> <li>Standardize CI / CD Toolchain: Collaborate with your peers in the engineering teams and build a business case for standardizing on a subset of CI / CD technologies and seek executive buy-ins. Focus on non-security benefits to gain traction and drive the message home. For example, \"Implementing a unified set of tools across teams improves collaboration and knowledge sharing, as all members work with the same processes and technologies. This standardization also leads to faster onboarding of new team members and easier skill transfer between projects, as there's only one set of tools to learn. Furthermore, a standardized toolchain reduces complexity and operational friction, simplifying maintenance and support while potentially leading to cost savings through consolidated licensing and training.\"</li> <li>Maintain Security Integration Toolkit: Develop and maintain a central repository of integration scripts and configurations for the approved CI/CD pipeline technologies, making it easier to implement security scanning consistently. Include clear guidelines and documentation to facilitate increased adoption by engineering teams. Ensure the repository is regularly updated to incorporate new CI/CD technologies or vendor updates.</li> </ul> </li> <li> <p>Preventing Bypass of Scanning Workflows: Due to the decentralized operating model for cloud, application and DevOps teams often persist admin privileges over their respective pipelines. This can pose challenges for enforcing scanning, as these teams can disable or bypass scanning steps. Therefore, when implementing scanning across the organization, it is crucial to design processes and solutions that minimize or restrict bypasses, ideally requiring security approval for any exceptions. Below are key considerations for this:</p> <ul> <li>Lockdown Important Touchpoints:<ul> <li>Isolate essential actions, such as deploying cloud resources or pushing images to registries, into reusable pipelines or workflows that are secured and can only be modified by a limited group of individuals. This ensures control over the critical touchpoints in the DevOps processes.</li> <li>Establish access controls so that deployments or image pushes can only occur through these tightly managed reusable workflows.</li> <li>Integrate security scanning and other requirements directly into these reusable workflows to ensure that necessary security measures are in place, making it difficult for teams to bypass security scans.</li> </ul> </li> <li>Leverage Native CI / CD Pipeline Capabilities:<ul> <li>Modern CI/CD platforms (e.g., Harness) offer policy management capabilities that allow organizations to define and enforce standardized practices across how their pipelines are created, managed, and executed. Leverage this policy-as-code feature to enforce security scanning (and any other requirements) as \u201crequired steps\u201d that need to be embedded for the successful creation and execution of pipelines. This provides reasonable assurance that pipelines are created with the necessary security guardrails.</li> </ul> </li> </ul> </li> <li> <p>Set Different Policies for Different Environments: Blanket security requirements (e.g., all critical, high, and medium issues must be remediated) can cause friction when enforced in lower environments as this inhibits developer productivity and increases the remediation burden for them. As such, it is advisable to have policies that have environmental context baked into them. For example, I have seen successful adoption of policies along the lines of:</p> <ul> <li>Dev &amp; Test Environments \u2014&gt; No critical issues</li> <li>Stg / Pre-Prod Environments \u2014&gt; No critical and high issues</li> <li>Prod Environments \u2014&gt; No critical, high, and medium issues</li> </ul> <p>There\u2019s potential for further granularity here based on factors such as workload type, business criticality, data sensitivity, and regulatory requirements. Another example I have seen for organizations running regulated workloads:</p> <ul> <li>Non-Regulated Workloads:<ul> <li>Dev &amp; Test Environments \u2014&gt; No critical issues</li> <li>Stg / Pre-Prod Environments \u2014&gt; No critical and high issues</li> <li>Prod Environments \u2014&gt; No critical, high, and medium issues </li> </ul> </li> <li>Regulated Workloads:<ul> <li>Dev &amp; Test Environments \u2014&gt; No critical and high issues</li> <li>Stg / Pre-Prod Environments \u2014&gt; No critical, high, and medium issues</li> <li>Prod Environments \u2014&gt; No critical, high, and medium issues</li> </ul> </li> </ul> </li> </ul>"},{"location":"Implementing%20CNAPP%3A%20Day%202%20Focus%20Areas/#container-image-scanning-and-sca","title":"Container Image Scanning and SCA","text":"<p>This includes scanning your container images and software packages during the build stage to identify vulnerabilities, misconfigurations, exposed secrets, etc.. The key considerations for image scanning and SCA are similar to the ones listed above for IaC &amp; secret scanning. </p>"},{"location":"Implementing%20CNAPP%3A%20Day%202%20Focus%20Areas/#capabilities-beyond-cspm","title":"Capabilities Beyond CSPM","text":"<p>As a recap from the previous blog, I expanded the definition of CSPM to include core CSPM because they are relatively straightforward to implement, deliver quick value, and have a similar path to operationalization. In this section, I will focus on the additional capabilities of CNAPP that build upon the insights and lessons learned - such as high-risk areas, cloud environment setup, landing zone design, naming conventions, tagging standards, etc. - from operationalizing CSPM. Below are some of the core CNAPP capabilities that extend beyond CSPM:</p> <ol> <li>Registry Scanning </li> <li>Cloud Infrastructure Entitlement Management (CIEM), Data Security Posture Management (DSPM) and Attack Surface Management </li> <li>Container and Kubernetes Security </li> <li>Cloud Detection and Response (CDR)</li> </ol>"},{"location":"Implementing%20CNAPP%3A%20Day%202%20Focus%20Areas/#registry-scanning","title":"Registry Scanning","text":"<p>This includes scanning your container registries to detect vulnerabilities and malware on images. This enables you to have visibility into images that:</p> <ul> <li>Have been pushed to the registries outside of the standard CI / CD pipelines  </li> <li>Have been running in the environment for a long period and have become vulnerable after the initial scan during the build phase</li> </ul> <p>If your organization uses cloud-native registries (e.g., Amazon Elastic Container Registry (ECR), Azure Container Registry (ACR)), CNAPP tools typically scan them without requiring additional configuration, as this feature is usually enabled by default during the initial setup. However, if you are utilizing a third-party registry (e.g., JFrog Artifactory), further configurations may be necessary for scanning. Below are key considerations for registry scanning:</p> <ul> <li> <p>Managing Volume of Issues: The number of issues identified can be quite substantial, as registries can become chaotic for several reasons:</p> <ul> <li>Teams may push numerous images and packages while only utilizing a small fraction of those for their workloads.</li> <li>The lack of a well-defined registry structure can make it difficult to track ownership and accountability.</li> <li>Registry access can be left widely open (i.e. no RBAC), allowing anyone to push to any location within the registry.</li> </ul> <p>Given these challenges, it is essential to invest efforts in correlating issues with actual running containers and workloads and prioritizing them for remediation, rather than requiring remediation for all identified issues.</p> </li> <li> <p>Assigning Remediation Ownership: The approach to assigning remediation ownership can vary significantly depending on your organization\u2019s operating model, due to the layered nature of containers.</p> <ul> <li> <p>If your organization has a central team (e.g., cloud engineering) responsible for maintaining golden base images (the base layer) that application and DevOps teams build upon with their specific layers, then it is essential to trace the issue back to the vulnerable layer and assign remediation ownership accordingly.</p> <ul> <li> <p>If the issue lies within the base layer, the effort required for remediation across the entire environment increases significantly for the following reasons:</p> <ul> <li>The central team must update or create a new golden base image that includes the fix.</li> <li>The consuming teams (i.e., application and DevOps teams) will need to redeploy their applications and workloads using this updated base layer.</li> </ul> <p>Doing this regularly requires organizations to have mature DevOps processes where teams understand the importance and need to constantly rehydrate their images. Furthermore, there should be good testing and dependency management programs in place to ensure that applications are thoroughly tested before these updates are released to production and that base image modifications do not cause any disruptions.</p> </li> <li> <p>If the issue is associated with the application layer, then the responsibility lies with the respective application / DevOps teams to remediate and redeploy their application / workload images. The operations around this are relatively easier because the burden lies exclusively with the application / DevOps teams and there are no dependencies on an enterprise team.</p> </li> </ul> </li> <li> <p>If your organization\u2019s setup is one where the concept of golden images does not exist and the application / DevOps teams own the entire lifecycle of the container images, then the ownership assignment and operations are similar to the previous point about managing issues at the application layer.</p> </li> </ul> </li> </ul>"},{"location":"Implementing%20CNAPP%3A%20Day%202%20Focus%20Areas/#ciem-dspm-and-attack-surface-management","title":"CIEM, DSPM, and Attack Surface Management","text":"<p>Although these are all broad and disparate categories, I have grouped them under one section because the approach to operationalizing each of these capabilities is similar. Once you are able to operationalize one capability, the strategy and process for the rest of the areas will be comparable. Below are the key considerations for these capabilities:</p> <ul> <li> <p>Current-State Understanding: If a comprehensive discovery exercise regarding current processes, environment setup, landing zone design, and other factors has not yet been conducted as part of operationalizing CSPM, it should be prioritized as the first step in expanding into these CNAPP areas. Below are example questions to give you insights into the kind of understanding that should be developed:</p> <ul> <li>What is the tagging standard in place, and how well is it adopted? Does the standard include details such as resource owner, application owner, and data classification?</li> <li>What is the naming convention for resources, and is it applied consistently? For example, Is there a clear method for distinguishing between cloud admin resources, developer resources, and security resources?</li> <li>Who is responsible for managing the lifecycle of the different roles and how is it done?</li> <li>What are the different 3rd parties that are expected to have access to the cloud environments?</li> <li>Is there a comprehensive asset inventory that tracks various subscriptions, accounts, and projects known or expected to have publicly exposed endpoints or sensitive data sources?</li> <li>What does the networking setup look like, and what approved patterns are in place for publicly exposing endpoints?</li> </ul> </li> <li> <p>Custom Developing &amp; Fine-Tuning Policies: Out-of-the-box (OOB) CNAPP policies are valuable for establishing a baseline understanding of the different types of detections available / possible. However, they lack the necessary context around the setup of the cloud environment to distinguish between different types of resources and accurately detect issues - such as over-permissive roles (cloud admin roles (expected) v/s EC2 instance roles (not expected)), presence of sensitive data sources (S3 bucket in regulated production account containing PII (expected) v/s S3 bucket in development containing PII (not expected)), and externally exposed endpoints (ALB in a production account expected to have internet exposure (expected) v/s ALB in development account (not expected)). Simply enabling these policies without any customization or fine-tuning can lead to false positives or a high volume of risk exceptions. Therefore, it is essential to prioritize customization and fine-tuning from the outset when operationalizing these CNAPP areas. Below are some of the customizations you should consider applying:</p> <ul> <li>Use the baseline CNAPP policies as a starting point and<ul> <li>Leverage resource tags, resource names, and accounts / subscription information to apply filtering on the base policies so that context-unaware requirements are not enforced (e.g., it is expected for cloud admins to have excessive (or admin) privileges in the environment but not for EC2 instance roles)</li> <li>Customize / modify the specific logic to tailor to your standards (e.g., if your organization's policy for inactive roles is 120 days instead of the OOB CNAPP policy of 90 days, then customize the policies accordingly).</li> </ul> </li> <li> <p>Develop a prioritized backlog of requirements and implement custom policies to identify violations to the design patterns that you have defined as an organization (e.g., resources in development should not access resources in production, resources in development should not be publicly exposed, resources in development should not contain sensitive data, etc.)</p> <p>Below are some examples to further illustrate the intent behind customization / fine-tuning of CNAPP policies:</p> <ul> <li>Identify all storage buckets and database instances containing PII, PCI, or PHI data that are missing the tag/label \u201cData Classification: Sensitive.\u201d</li> <li>Identify all storage buckets and database instances with PII, PCI, or PHI data that belong to the development or test Organizational Unit (OU).</li> <li>Identify all roles that do not have the name \u201ccloud-admin-roles\u201d but possess admin privileges.</li> <li>Identify all publicly exposed resources that do not belong to the subscription/account labeled \u201cexternal-access-account.\"</li> </ul> </li> </ul> </li> <li> <p>Correlating Findings Across Different CNAPP Areas: Once you have familiarized yourself with the environment and have some experience in developing CNAPP policies across the above mentioned different areas, you can develop more complex detections that string together issues from the different capability areas to uncover more interesting insights. Some examples include:</p> <ul> <li>IAM roles in the development OU have cross-account access to roles in production OU which have admin privileges on storage buckets or database instances that have sensitive data</li> <li>Lambda functions are publicly exposed and have administrative privileges on cloud-admin-roles</li> <li>Lambda functions are publicly exposed and contain critical vulnerabilities with known public exploit</li> </ul> </li> </ul>"},{"location":"Implementing%20CNAPP%3A%20Day%202%20Focus%20Areas/#container-and-kubernetes-security","title":"Container and Kubernetes Security:","text":"<p>This section addresses capabilities focused on securing actively running containerized workloads and Kubernetes clusters, specifically Cloud Workload Protection Platform (CWPP), Kubernetes Security Posture Management (KSPM), and Kubernetes Admissions Controller. It does not encompass capabilities like helm chart/Kubernetes manifest file scanning, Dockerfile scanning, and container image scanning, as these are more \u201cpre-deployment\u201d and have already been discussed above. Additionally, the rationale for grouping these capabilities is that they are simalar to deployment of security agents (e.g., EDR, SIEM forwarders, etc.) in that they need to be enabled on a per container, node, or cluster level. This decentralized approach can lead to increased operational complexity when scaling these capabilities across large platform footprints. Below are some key considerations for ensuring the runtime security of containers and Kubernetes clusters:</p> <ul> <li>Prioritize Critical Workloads: Implementing these capabilities across the entire organization can become a high-touch effort, especially if multiple teams serve as platform admins for their respective applications or business units. Therefore, consider focusing the rollout of these capabilities on a prioritized set of workloads or clusters instead of attempting to cover all containers or clusters. The strategy for identifying prioritized workloads will vary by organization, but production workloads, business critical workloads, externally exposed workloads, and those interacting with sensitive data are good candidates for prioritization.  </li> <li>Maintain Deployment Templates: For all the different flavors of orchestration platforms that are in-scope, develop and maintain instruction sets, guidance materials, and deployment scripts that can be readily used by the platform teams to deploy the KSPM connectors, admissions controllers, and CWPP sensors / agents. Additionally, ensure these artifacts are regularly updated to align with vendor releases and upgrades.</li> </ul>"},{"location":"Implementing%20CNAPP%3A%20Day%202%20Focus%20Areas/#cloud-detection-and-response-cdr","title":"Cloud Detection and Response (CDR)","text":"<p>This is a relatively newer area for CNAPP that includes capabilities such as malware and threat detection, cloud events analysis, forensics collection, and automated responses. While I have primarily engaged in proofs-of-concept, utilizing the platform for initial investigations and information gathering, and deploying automated responses for simple use cases (such as quarantining resources, blocking public access, enabling encryption, upgrading EC2 instances to IMDSv2, etc.), I am excited to see how this field evolves. The integration of cloud events and CNAPP detection within a single platform seems very powerful. This will give us the ability to potentially start developing policies that are based on actual activities that are happening in the cloud environments coupled with the visibility and context that a CNAPP tool can provide. This could help offload some of the query development that otherwise happens within SIEM platforms, positioning CNAPP as the first line of detection and alerting. These alerts can then be forwarded to SIEM for further correlation and analysis. This approach has the added advantage of relying on the cloud security team\u2019s expertise and understanding of the environment,  potentially resulting in more accurate alerts and fewer false positives. Below are example use-cases that could be possible by integrating cloud events with CNAPP:</p> <ul> <li>Identify all cloud IAM roles in development OU that attempted to access roles in production OU  </li> <li>Identify all cloud IAM roles that attempted to access buckets and database instances with sensitive data and had a high (above 70%) failure rate  </li> <li>Identify all cloud IAM roles that performed read actions on a high (above 15) number of services in the production OU in the last 24 hours  </li> <li>Identify all EC2 instances that are publicly exposed and attempted to create an IAM user with access keys and admin privileges</li> </ul>"},{"location":"Implementing%20CNAPP%3A%20Day%202%20Focus%20Areas/#operational-workflows-and-technology-integrations","title":"Operational Workflows and Technology Integrations","text":"<p>Operational workflows and technology integrations are important in ensuring that the insights generated by CNAPP tools are actioned efficiently and timely, thereby bridging the gap between identifying security issues and practical risk mitigation. This involves aspects such as:</p> <ul> <li>Defining remediation requirements by environment and criticality (see details in section IaC and Secret Scanning)</li> <li>Classifying issues and routing them to the appropriate remediation owners  </li> <li>Setting up integrations with existing tools and workflows  </li> <li>Providing remediation support  </li> <li>Reporting on SLA compliance and overall security posture</li> </ul> <p>While the specific workflows and integrations will differ for each organization based on factors such as size, existing toolchain, cloud and DevOps maturity, release and change management processes, etc., my experience working with various organizations around the operationalization of CNAPP issues has revealed the below key principles:</p> <ul> <li>Focus on developing solutions that are \u201cgood enough\u201d and work for your organization rather than overengineering to build the perfect workflows and integrations.   </li> <li>Foster a partnership with application and development teams by meeting them halfway in how they prefer to receive issue communications (e.g., email, Teams, Jira, Slack).  </li> <li>Avoid imposing your ideal scenario on them. This doesn\u2019t mean accommodating every tool; rather, concentrate on the tools your organization predominantly uses and allow teams to choose their preferences from that list.  </li> <li>Continuously seek feedback from the teams to enhance processes and integrations accordingly.</li> </ul> <p>Below is a visual that provides a high-level overview of classifying different types of issues, identifying responsible stakeholders, and setting up integrations for effective communication, tracking, and management of issues:  </p> <p></p> <p>In summary, addressing day 2 focus areas for CNAPP is essential for maximizing ROI and enhancing the overall maturity of the cloud security program. Prioritizing pre-deployment security capabilities - such as IaC, secret, and container image scanning - ensures vulnerabilities and misconfigurations are identified and remediated early in the development lifecycle, conserving valuable time and resources. By operationalizing capabilities beyond traditional CSPM, organizations gain increased visibility into critical security domains, including identity and access management, data security, network protection, attack surface management, container and workload security, and threat detection and response. Establishing clear operational workflows and integrating with existing tools - such as ticketing and messaging platforms - transforms security insights into actionable information that can be seamlessly incorporated into daily processes. </p> <p>Ultimately, organizations must commit to continually updating their capabilities to stay ahead of a dynamic threat landscape, recognizing that CNAPP implementation is a journey, not a one-time event. A journey that requires organizations to always be on the frontfoot and constantly update and improve existing capabilities, implement new capabilities, and seek operational efficiencies to mitigate risks and adapt to emerging threats with reasonable assurance.</p> <p>PS: CNAPP is a constantly evolving area where new capabilities are introduced regularly. The most recent announcement that made news was Wiz's release of \"Wiz Code\" that seeks to combine DevSecOps, Application Security Application Security Posture Management (ASPM), and CNAPP capabilities into a singular platform to provide unified visibility and security from \u201ccode-to-cloud\u201d. However, the intent of this blog is to focus on the core CNAPP capabilities that are available across most vendors. </p>"}]}